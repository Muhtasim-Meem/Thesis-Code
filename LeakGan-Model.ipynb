{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.8.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: spacy==3.7.5 in f:\\python\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: regex==2023.10.3 in f:\\python\\lib\\site-packages (2023.10.3)\n",
      "Requirement already satisfied: click in f:\\python\\lib\\site-packages (from nltk==3.8.1) (8.1.8)\n",
      "Requirement already satisfied: joblib in f:\\python\\lib\\site-packages (from nltk==3.8.1) (1.4.2)\n",
      "Requirement already satisfied: tqdm in f:\\python\\lib\\site-packages (from nltk==3.8.1) (4.66.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (0.15.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (3.1.5)\n",
      "Requirement already satisfied: setuptools in f:\\python\\lib\\site-packages (from spacy==3.7.5) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from spacy==3.7.5) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in f:\\python\\lib\\site-packages (from spacy==3.7.5) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in f:\\python\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in f:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in f:\\python\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\python\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in f:\\python\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in f:\\python\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk==3.8.1) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in f:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in f:\\python\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in f:\\python\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in f:\\python\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\python\\lib\\site-packages (from jinja2->spacy==3.7.5) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in f:\\python\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\python\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5) (2.18.0)\n",
      "Requirement already satisfied: wrapt in f:\\python\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.5) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\python\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5) (0.1.2)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "Successfully installed nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==2.1.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\n",
      "ERROR: No matching distribution found for torch==2.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.14.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.14.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm==4.66.0 in f:\\python\\lib\\site-packages (4.66.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from tqdm==4.66.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sacrebleu==2.4.0 in f:\\python\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: rouge-score==0.1.2 in f:\\python\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: portalocker in f:\\python\\lib\\site-packages (from sacrebleu==2.4.0) (3.1.1)\n",
      "Requirement already satisfied: regex in f:\\python\\lib\\site-packages (from sacrebleu==2.4.0) (2023.10.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in f:\\python\\lib\\site-packages (from sacrebleu==2.4.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in f:\\python\\lib\\site-packages (from sacrebleu==2.4.0) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from sacrebleu==2.4.0) (0.4.6)\n",
      "Requirement already satisfied: lxml in f:\\python\\lib\\site-packages (from sacrebleu==2.4.0) (5.3.0)\n",
      "Requirement already satisfied: absl-py in f:\\python\\lib\\site-packages (from rouge-score==0.1.2) (2.1.0)\n",
      "Requirement already satisfied: nltk in f:\\python\\lib\\site-packages (from rouge-score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: click in f:\\python\\lib\\site-packages (from nltk->rouge-score==0.1.2) (8.1.8)\n",
      "Requirement already satisfied: joblib in f:\\python\\lib\\site-packages (from nltk->rouge-score==0.1.2) (1.4.2)\n",
      "Requirement already satisfied: tqdm in f:\\python\\lib\\site-packages (from nltk->rouge-score==0.1.2) (4.66.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->sacrebleu==2.4.0) (306)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb==0.15.9\n",
      "  Using cached wandb-0.15.9-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in f:\\python\\lib\\site-packages (from wandb==0.15.9) (8.1.8)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.15.9)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in f:\\python\\lib\\site-packages (from wandb==0.15.9) (2.32.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from wandb==0.15.9) (6.0.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb==0.15.9)\n",
      "  Downloading sentry_sdk-2.20.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb==0.15.9)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting PyYAML (from wandb==0.15.9)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting pathtools (from wandb==0.15.9)\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Muhtasim\\AppData\\Local\\Temp\\pip-install-13o8rb32\\pathtools_60a4e9c2a71e4e9b860420486111b8af\\setup.py\", line 25, in <module>\n",
      "          import imp\n",
      "      ModuleNotFoundError: No module named 'imp'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "#Requirements:\n",
    "\n",
    "%pip install nltk==3.8.1 spacy==3.7.5 regex==2023.10.3\n",
    "\n",
    "%pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0\n",
    "%pip install tensorflow==2.14.0 tensorflow-gan==2.0.0\n",
    "\n",
    "%pip install tqdm==4.66.0\n",
    "%pip install sacrebleu==2.4.0 rouge-score==0.1.2\n",
    "\n",
    "%pip install wandb==0.15.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in f:\\python\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in f:\\python\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in f:\\python\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in f:\\python\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in f:\\python\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\python\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\python\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in f:\\python\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in f:\\python\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in f:\\python\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\python\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in f:\\python\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm in f:\\python\\lib\\site-packages (4.66.0)\n",
      "Requirement already satisfied: numpy in f:\\python\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: click in f:\\python\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in f:\\python\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\python\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in f:\\python\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in f:\\python\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in f:\\python\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in f:\\python\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in f:\\python\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "%pip install nltk tqdm numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: indic-nlp-library in f:\\python\\lib\\site-packages (0.92)\n",
      "Requirement already satisfied: sphinx-argparse in f:\\python\\lib\\site-packages (from indic-nlp-library) (0.5.2)\n",
      "Requirement already satisfied: sphinx-rtd-theme in f:\\python\\lib\\site-packages (from indic-nlp-library) (3.0.2)\n",
      "Requirement already satisfied: morfessor in f:\\python\\lib\\site-packages (from indic-nlp-library) (2.0.6)\n",
      "Requirement already satisfied: pandas in f:\\python\\lib\\site-packages (from indic-nlp-library) (2.2.3)\n",
      "Requirement already satisfied: numpy in f:\\python\\lib\\site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\python\\lib\\site-packages (from pandas->indic-nlp-library) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Requirement already satisfied: sphinx>=5.1.0 in f:\\python\\lib\\site-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\n",
      "Requirement already satisfied: docutils>=0.19 in f:\\python\\lib\\site-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in f:\\python\\lib\\site-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.5)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.13 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
      "Requirement already satisfied: alabaster>=0.7.14 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.30.0 in f:\\python\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\muhtasim\\appdata\\roaming\\python\\python312\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\python\\lib\\site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\python\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\python\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\python\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\python\\lib\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement bangla-tokenizer (from versions: none)\n",
      "ERROR: No matching distribution found for bangla-tokenizer\n"
     ]
    }
   ],
   "source": [
    "%pip install indic-nlp-library\n",
    "%pip install bangla-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "বজাও রে মোহন বাঁশি।\n",
      "সারা দিবসক\n",
      "বিরহদহনদুখ,\n",
      "মরমক তিয়াষ নাশি।\n",
      "রিঝমনভেদন\n",
      "বাঁশরিবাদন\n",
      "কঁহা শিখলি রে কান?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# file path to a poems text file\n",
    "file_path = r\"F:\\Thesis-Code\\poem.txt\" \n",
    "\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        print(content[:100])  \n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path}. Please check the path and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Poems Preview:\n",
      "বজাও রে মোহন বাঁশি । \n",
      "\n",
      "সারা দিবসক\n",
      "\n",
      "বিরহদহনদুখ , \n",
      "\n",
      "মরমক তিয়াষ নাশি । \n",
      "\n",
      "রিঝমনভেদন\n",
      "\n",
      "বাঁশরিবাদন\n",
      "\n",
      "কঁহা শিখলি রে কান ? \n",
      "\n",
      "হানে থিরথির\n",
      "\n",
      "মরমঅবশকর\n",
      "\n",
      "লহু লহু মধুময় বাণ । \n",
      "\n",
      "ধসধস করতহ\n",
      "\n",
      "উরহ বিয়াকুলু , \n",
      "\n",
      "ঢুলু ঢুলু অবশনয়ান ; \n",
      "\n",
      "কত কত বরষক\n",
      "\n",
      "বাত সোঁয়ারয় , \n",
      "\n",
      "অধীর করয় পরান । \n",
      "\n",
      "কত শত আশা\n",
      "\n",
      "পূরল না বঁধু , \n",
      "\n",
      "কত সুখ করল পয়ান । \n",
      "\n",
      "পহু গো কত শত\n",
      "\n",
      "পীরিতযাতন\n",
      "\n",
      "হিয়ে বিঁধাওল বাণ । \n",
      "\n",
      "হৃদয় উদাসয় , \n",
      "\n",
      "নয়ন উছাসয়\n",
      "\n",
      "দারুণ\n",
      "\n",
      "মধুময় গান । \n",
      "\n",
      "সাধ যায় বঁধূ , \n",
      "\n",
      "যমুনাবারিম\n",
      "\n",
      "ডারিব\n",
      "\n",
      "দগধপরান । \n",
      "\n",
      "সাধ যায় পহু , \n",
      "\n",
      "রাখি চরণ তব\n",
      "\n",
      "হৃদয়মাঝ\n",
      "\n",
      "হৃদয়েশ , \n",
      "\n",
      "হৃদয়জুড়াওন\n",
      "\n",
      "বদনচন্দ্র তব\n",
      "\n",
      "হেরব\n",
      "\n",
      "জীবনশেষ । \n",
      "\n",
      "সাধ যায় , ইহ\n",
      "\n",
      "চন্দ্রমকিরণে\n",
      "\n",
      "কুসুমিত\n",
      "\n",
      "কুঞ্জবিতানে\n",
      "\n",
      "বসন্তবায়ে\n",
      "\n",
      "প্রাণ মিশায়ব\n",
      "\n",
      "বাঁশিক সুমধুর গানে । \n",
      "\n",
      "প্রাণ ভৈবে মঝু\n",
      "\n",
      "বেণুগীতময় , \n",
      "\n",
      "রাধাময় তব\n",
      "\n",
      "বেণু । \n",
      "\n",
      "জয় জয় মাধব , \n",
      "\n",
      "জয় জয় রাধা , \n",
      "\n",
      "চরণে\n",
      "\n",
      "প্রণমে ভানু । \n",
      "\n",
      "শুনহ শুনহ বালিকা , \n",
      "\n",
      "রাখ কুসুমমালিকা , \n",
      "\n",
      "কুঞ্জ কুঞ্জ ফেরনু সখি শ্যামচন্দ্র নাহি রে । \n",
      "\n",
      "দুলই কুসুমমুঞ্জরী , \n",
      "\n",
      "ভমর ফিরই গুঞ্জরি , \n",
      "\n",
      "অলস যমুনা বহয়ি যায় ললিত গীত গাহি রে । \n",
      "\n",
      "শশিসনাথ যামিনী , \n",
      "\n",
      "বিরহবিধুর কামিনী , \n",
      "\n",
      "কুসুমহার ভইল ভার— হৃদয় তার দাহিছে । \n",
      "\n",
      "অধর উঠই কাঁপিয়া\n",
      "\n",
      "সখিকরে কর আপিয়া , \n",
      "\n",
      "কুঞ্জভবনে পাপিয়া কাহে গীত গাহিছে । \n",
      "\n",
      "মৃদু সমীর সঞ্চলে\n",
      "\n",
      "হরয়ি শিথিল অঞ্চলে , \n",
      "\n",
      "চকিত হৃদয় চঞ্চলে কাননপথ চাহি রে । \n",
      "\n",
      "কুঞ্জপানে হেরিয়া\n",
      "\n",
      "অশ্রুবারি ডারিয়া\n",
      "\n",
      "ভানু গায় শূন্যকুঞ্জ শ্যামচন্দ্র নাহি রে ! \n",
      "\n",
      "হৃদয়ক সাধ মিশাওল হৃদয়ে , \n",
      "\n",
      "কন্ঠে বিমলিন মালা । \n",
      "\n",
      "বিরহবিষে দহি বহি গল রয়নী , \n",
      "\n",
      "নহি নহি আওল কালা । \n",
      "\n",
      "বুঝনু বুঝনু সখি বিফল বিফল সব , \n",
      "\n",
      "বিফল এ পীরিতি লেহা —\n",
      "\n",
      "বিফল রে এ মঝু জীবন যৌবন , \n",
      "\n",
      "বিফল রে এ মঝু দেহা ! \n",
      "\n",
      "চল সখি গৃহ চল , মঞ্চ নয়নজল , \n",
      "\n",
      "চল সখি চল গৃহকাজে , \n",
      "\n",
      "মালতিমালা রাখহ বালা , \n",
      "\n",
      "ছি ছি সখি মরু মরু লাজে । \n",
      "\n",
      "সখি লো দারুণ আধিভরাতুর\n",
      "\n",
      "এ তরুণ যৌবন মোর , \n",
      "\n",
      "সখি লো দারুণ প্রণয়হলাহল\n",
      "\n",
      "জীবন করল অঘোর । \n",
      "\n",
      "তৃষিত প্রাণ মম দিবসযামিনী\n",
      "\n",
      "শ্যামক দরশন আশে , \n",
      "\n",
      "আকুল জীবন থেহ ন মানে , \n",
      "\n",
      "অহরহ জ্বলত হুতাশে । \n",
      "\n",
      "সজনি , সত্য কহি তোয় , \n",
      "\n",
      "খোয়াব কব হম শ্যামক প্রেম\n",
      "\n",
      "সদা ডর লাগয়ে মোয় । \n",
      "\n",
      "হিয়ে হিয়ে অব রাখত মাধব , \n",
      "\n",
      "সো দিন আসব সখি রে - \n",
      "\n",
      "বাত ন বোলবে , বদন ন হেরবে , \n",
      "\n",
      "মরিব হলাহল ভখি রে । \n",
      "\n",
      "ঐস বৃথা ভয় না কর বালা , \n",
      "\n",
      "ভানু নিবেদয় চরণে , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load poems from a file\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "\n",
    "\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    poems_raw = file.readlines()\n",
    "poems = []\n",
    "for line in poems_raw:\n",
    "    if line.strip():  \n",
    "        tokenized_line = indic_tokenize.trivial_tokenize(line.lower(), lang='bn')\n",
    "        poems.append(' '.join(tokenized_line))\n",
    "print(\"Tokenized Poems Preview:\")\n",
    "for poem in poems[:100]:  \n",
    "    print(poem)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Muhtasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Epoch 1: 100%|██████████| 1477/1477 [3:24:31<00:00,  8.31s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Generator Loss = 12444.8962, Discriminator Loss = 62.3104, F1 Score = 0.0359, Precision = 1.0000, Recall = 0.0183, Accuracy = 1.83%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 249\u001b[0m\n\u001b[0;32m    246\u001b[0m         generated_data \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39msample(start_token\u001b[38;5;241m=\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mword_to_idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_length\u001b[38;5;241m=\u001b[39mseq_length, num_samples\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    247\u001b[0m         val_predictions\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([train_dataset\u001b[38;5;241m.\u001b[39midx_to_word[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m seq]) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m generated_data\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()])\n\u001b[1;32m--> 249\u001b[0m     bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_references\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation BLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m lr_scheduler_g\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[1], line 139\u001b[0m, in \u001b[0;36mcompute_bleu\u001b[1;34m(predictions, references)\u001b[0m\n\u001b[0;32m    137\u001b[0m bleu_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(references, predictions):\n\u001b[1;32m--> 139\u001b[0m     bleu_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43msentence_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmooth_fn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(bleu_scores)\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_bleu\u001b[39m(\n\u001b[0;32m     21\u001b[0m     references,\n\u001b[0;32m     22\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     auto_reweigh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m ):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    :rtype: float / list(float)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reweigh\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[0;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "#--------------------------------------LeakGan Modle---------------------------------------#-----------Error Free code------------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, poems, vocab, seq_length):\n",
    "        self.poems = poems\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = word_tokenize(self.poems[idx].lower())[:self.seq_length]\n",
    "        input_ids = [self.word_to_idx.get(word, self.word_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.word_to_idx.get(word, self.word_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        input_ids = input_ids + [self.word_to_idx[\"<pad>\"]] * (self.seq_length - len(input_ids))\n",
    "        target_ids = target_ids + [self.word_to_idx[\"<pad>\"]] * (self.seq_length - len(target_ids))\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# Load and preprocess data\n",
    "with open(r\"F:\\Thesis-Code\\poem.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poems = file.readlines()\n",
    "\n",
    "poems = [' '.join(word_tokenize(line.strip())) for line in poems if line.strip()]\n",
    "train_data, val_data = train_test_split(poems, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "all_words = set(word for line in poems for word in line.split())\n",
    "vocab = [\"<pad>\", \"<unk>\", \"<start>\", \"<end>\"] + sorted(all_words)\n",
    "vocab_size = len(vocab)\n",
    "seq_length = 50\n",
    "\n",
    "train_dataset = PoemDataset(train_data, vocab, seq_length)\n",
    "val_dataset = PoemDataset(val_data, vocab, seq_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#Generator\n",
    "class AdvancedGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, leak_info_size, dropout=0.5):\n",
    "        super(AdvancedGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "       \n",
    "        self.lstm = nn.LSTM(embed_size + leak_info_size, hidden_size, batch_first=True, dropout=dropout, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.leak_info_fc = nn.Linear(hidden_size, leak_info_size)\n",
    "\n",
    "    def forward(self, x, leak_info):\n",
    "        embedded = self.embedding(x)\n",
    "        augmented_input = torch.cat((embedded, leak_info.unsqueeze(1).repeat(1, embedded.size(1), 1)), dim=-1)\n",
    "        lstm_out, _ = self.lstm(augmented_input)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, start_token, max_length, num_samples=100, temperature=0.1, top_k=50):\n",
    "        poems = torch.full((num_samples, 1), start_token, dtype=torch.long).to(device)\n",
    "        batch_size = poems.size(0)\n",
    "        leak_info = torch.zeros(batch_size, self.leak_info_fc.out_features).to(device)\n",
    "\n",
    "        self.eval()\n",
    "        for _ in range(max_length - 1):\n",
    "            embedded = self.embedding(poems[:, -1:])\n",
    "            augmented_input = torch.cat((embedded, leak_info.unsqueeze(1)), dim=-1)\n",
    "            lstm_out, _ = self.lstm(augmented_input)\n",
    "            logits = self.fc(lstm_out).squeeze(1) / temperature\n",
    "\n",
    "            top_k_vals, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "            probabilities = torch.softmax(top_k_vals, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            next_token = top_k_indices.gather(-1, next_token)\n",
    "\n",
    "            poems = torch.cat([poems, next_token], dim=-1)\n",
    "            leak_info = torch.tanh(self.leak_info_fc(lstm_out[:, -1, :]))\n",
    "\n",
    "        return poems\n",
    "#Discriminator\n",
    "class AdvancedDiscriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.5):\n",
    "        super(AdvancedDiscriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "       \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_out[:, -1, :])\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "leak_info_size = 128\n",
    "generator = AdvancedGenerator(vocab_size, embed_size=256, hidden_size=512, leak_info_size=leak_info_size).to(device)\n",
    "discriminator = AdvancedDiscriminator(vocab_size, embed_size=256, hidden_size=512).to(device)\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.9, 0.98))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.9, 0.98))\n",
    "\n",
    "criterion_gen = nn.CrossEntropyLoss()\n",
    "criterion_disc = nn.BCELoss()\n",
    "\n",
    "lr_scheduler_g = optim.lr_scheduler.StepLR(g_optimizer, step_size=10, gamma=0.5)\n",
    "lr_scheduler_d = optim.lr_scheduler.StepLR(d_optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "#compute bleu score\n",
    "def compute_bleu(predictions, references):\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        bleu_scores.append(sentence_bleu([word_tokenize(ref)], word_tokenize(pred), smoothing_function=smooth_fn))\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def generate_poem(generator, start_token, max_length=50, num_samples=1, temperature=1.0, top_k=50):\n",
    "    \"\"\"\n",
    "    Generate a poem using the trained generator and format it line by line.\n",
    "    \"\"\"\n",
    "    \n",
    "    generated_sequences = generator.sample(start_token=start_token, max_length=max_length, num_samples=num_samples,\n",
    "                                           temperature=temperature, top_k=top_k)\n",
    "\n",
    "    generated_poems = []\n",
    "    for seq in generated_sequences:\n",
    "        words = []\n",
    "        for idx in seq:\n",
    "            idx = idx.item() if isinstance(idx, torch.Tensor) else idx\n",
    "            if idx < len(train_dataset.idx_to_word):\n",
    "                word = train_dataset.idx_to_word[idx]\n",
    "                if word not in [\"<start>\", \"<end>\", \"<unk>\", \"<pad>\"]:\n",
    "                    words.append(word)\n",
    "            else:\n",
    "                words.append(\"<unk>\")\n",
    "\n",
    "      \n",
    "        poem = \" \".join(words).strip()\n",
    "        poem_lines = []\n",
    "        words_per_line = 6  \n",
    "        for i in range(0, len(words), words_per_line):\n",
    "            line = \" \".join(words[i:i + words_per_line])\n",
    "            poem_lines.append(line)\n",
    "\n",
    "        formatted_poem = \"\\n\".join(poem_lines).strip()\n",
    "        generated_poems.append(formatted_poem)\n",
    "\n",
    "    return generated_poems\n",
    "\n",
    "#numbers of epochs\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    total_gen_loss, total_disc_loss = 0, 0\n",
    "    total_correct, total_samples = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        leak_info = torch.zeros(batch_size, leak_info_size).to(device)\n",
    "        gen_logits = generator(inputs, leak_info)\n",
    "\n",
    "        fake_data = generator.sample(start_token=train_dataset.word_to_idx[\"<start>\"], max_length=seq_length, num_samples=batch_size).to(device)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        real_preds = discriminator(inputs)\n",
    "        fake_preds = discriminator(fake_data)\n",
    "\n",
    "        d_loss = criterion_disc(real_preds, real_labels) + criterion_disc(fake_preds, fake_labels)\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        fake_preds = discriminator(fake_data)\n",
    "        g_loss = criterion_disc(fake_preds, real_labels)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        total_gen_loss += g_loss.item()\n",
    "        total_disc_loss += d_loss.item()\n",
    "\n",
    "        all_preds.extend(torch.round(fake_preds.detach()).cpu().numpy())\n",
    "        all_labels.extend(real_labels.cpu().numpy())\n",
    "\n",
    "        total_correct += (torch.round(fake_preds.detach()) == real_labels).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    total_accuracy = total_correct / total_samples * 100\n",
    "    train_f1 = f1_score(all_labels, all_preds, zero_division=1)\n",
    "    train_precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "    train_recall = recall_score(all_labels, all_preds, zero_division=1)\n",
    "\n",
    "    g_losses.append(total_gen_loss)\n",
    "    d_losses.append(total_disc_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Generator Loss = {total_gen_loss:.4f}, Discriminator Loss = {total_disc_loss:.4f}, \"\n",
    "          f\"F1 Score = {train_f1:.4f}, Precision = {train_precision:.4f}, Recall = {train_recall:.4f}, \"\n",
    "          f\"Accuracy = {total_accuracy:.2f}%\")\n",
    "\n",
    "  \n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = []\n",
    "        val_references = val_data\n",
    "        for inputs, _ in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            leak_info = torch.zeros(inputs.size(0), leak_info_size).to(device)\n",
    "            generated_data = generator.sample(start_token=train_dataset.word_to_idx[\"<start>\"], max_length=seq_length, num_samples=inputs.size(0))\n",
    "            val_predictions.extend([' '.join([train_dataset.idx_to_word[idx] for idx in seq]) for seq in generated_data.cpu().numpy()])\n",
    "\n",
    "        bleu_score = compute_bleu(val_predictions, val_references)\n",
    "        print(f\"Validation BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "    lr_scheduler_g.step()\n",
    "    lr_scheduler_d.step()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(g_losses, label=\"Generator Loss\")\n",
    "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Generator and Discriminator Loss during Training')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "start_token = train_dataset.word_to_idx[\"<start>\"]\n",
    "generated_poem = generate_poem(generator, start_token, max_length=50, num_samples=1, temperature=1.0, top_k=50)\n",
    "\n",
    "\n",
    "if generated_poem:\n",
    "    print(\"\\nGenerated Poem:\\n\")\n",
    "    print(generated_poem[0])  \n",
    "else:\n",
    "    print(\"No poem generated. Check the token indices or sampling settings.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
