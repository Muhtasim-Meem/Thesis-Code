{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Muhtasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Generator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1477 [00:17<7:19:45, 17.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m--> 144\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion_gen(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    147\u001b[0m     g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding\n\u001b[0;32m     83\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(embedded)\n\u001b[1;32m---> 84\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-----------------------------Custom Mode------------------------------------#Final---version-----------------------------\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "nltk.download('punkt')\n",
    "import random\n",
    "\n",
    "\n",
    "# 1. Dataset Preparation\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, poems, vocab, seq_length):\n",
    "        self.poems = poems\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.poems[idx].split()\n",
    "        words = words[:self.seq_length]  # Limit the length of the poem to seq_length\n",
    "        input_ids = [self.word_to_idx.get(word, self.word_to_idx[\"<unk>\"]) for word in words[:-1]]\n",
    "        target_ids = [self.word_to_idx.get(word, self.word_to_idx[\"<unk>\"]) for word in words[1:]]\n",
    "\n",
    "        # Padding the sequences to ensure they have the same length\n",
    "        input_ids = input_ids + [self.word_to_idx[\"<pad>\"]] * (self.seq_length - len(input_ids))\n",
    "        target_ids = target_ids + [self.word_to_idx[\"<pad>\"]] * (self.seq_length - len(target_ids))\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "# Custom collate_fn for padding sequences dynamically in each batch\n",
    "def collate_fn(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    target_ids = torch.stack(target_ids, dim=0)\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# Load and preprocess data\n",
    "with open(r\"F:\\LeakGan_thesis\\poem.txt\\poem.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poems = file.readlines()\n",
    "\n",
    "poems = [' '.join(word_tokenize(line.lower())) for line in poems if line.strip()]\n",
    "train_data, val_data = train_test_split(poems, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = set(word for line in poems for word in line.split())\n",
    "vocab = [\"<pad>\", \"<unk>\", \"<start>\"] + sorted(all_words)  # Add \"<start>\" token\n",
    "vocab_size = len(vocab)\n",
    "seq_length = 50\n",
    "\n",
    "train_dataset = PoemDataset(train_data, vocab, seq_length)\n",
    "val_dataset = PoemDataset(val_data, vocab, seq_length)\n",
    "\n",
    "# Use the custom collate_fn in the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 2. Generator (Transformer-based architecture)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, word_to_idx):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_length, embed_size))  # Positional Encoding\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=8, dropout=0.3, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) + self.positional_encoding\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        logits = self.fc(transformer_output)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, start_token, max_length, num_lines=1, temperature=1.0, top_k=50):\n",
    "        self.eval()\n",
    "        poem = [[start_token] for _ in range(num_lines)]\n",
    "        for _ in range(max_length-1):\n",
    "            inputs = torch.tensor([line[-1] for line in poem], dtype=torch.long).unsqueeze(1)\n",
    "            logits = self(inputs)\n",
    "            logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-k sampling\n",
    "            topk_values, topk_indices = torch.topk(probabilities, top_k)\n",
    "            next_tokens = torch.multinomial(topk_values, 1).squeeze(1)\n",
    "            next_tokens = topk_indices.gather(1, next_tokens.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            for i, token in enumerate(next_tokens):\n",
    "                poem[i].append(token.item())\n",
    "                if token == self.word_to_idx[\"<pad>\"]:\n",
    "                    poem[i] = poem[i][:len(poem[i]) - 1]\n",
    "\n",
    "        poem_tensor = torch.tensor([line[:max_length] + [self.word_to_idx[\"<pad>\"]] * (max_length - len(line)) for line in poem], dtype=torch.long)\n",
    "      # Ensure it’s a tensor\n",
    "        return poem_tensor\n",
    "\n",
    "# 3. Discriminator (Transformer-based architecture)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_length, embed_size))  # Positional Encoding\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=8, dropout=0.3, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n",
    "        self.fc = nn.Linear(embed_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) + self.positional_encoding\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        logits = self.fc(transformer_output[:, -1, :])\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(vocab_size, embed_size=256, hidden_size=512, word_to_idx=train_dataset.word_to_idx)\n",
    "discriminator = Discriminator(vocab_size, embed_size=256, hidden_size=512)\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.9, 0.98))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.9, 0.98))  # Lowered d_optimizer learning rate\n",
    "\n",
    "# Loss functions\n",
    "criterion_gen = nn.CrossEntropyLoss()\n",
    "criterion_disc = nn.BCELoss()\n",
    "\n",
    "# 4. Pretrain Generator\n",
    "print(\"Pretraining Generator...\")\n",
    "num_epochs = 3# Increased epochs for more training\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        logits = generator(inputs)\n",
    "        loss = criterion_gen(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 5. Pretrain Discriminator\n",
    "print(\"Pretraining Discriminator...\")\n",
    "for epoch in range(num_epochs):\n",
    "    discriminator.train()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    for inputs, _ in tqdm(train_loader):\n",
    "        # Real labels\n",
    "        real_labels = torch.ones(inputs.size(0), 1)\n",
    "\n",
    "        # Generate fake data using the generator\n",
    "        fake_data = generator.sample(start_token=train_dataset.word_to_idx[\"<start>\"], max_length=seq_length, num_lines=inputs.size(0))\n",
    "        fake_data = fake_data.clone().detach()  # Shape: [batch_size, seq_length]\n",
    "\n",
    "        # Ensure fake data matches the sequence length (seq_length = 50)\n",
    "        if fake_data.size(1) < seq_length:\n",
    "            # Pad if needed\n",
    "            padding_size = (seq_length - fake_data.size(1))\n",
    "            fake_data = torch.nn.functional.pad(fake_data, (0, padding_size), value=train_dataset.word_to_idx[\"<pad>\"])\n",
    "        elif fake_data.size(1) > seq_length:\n",
    "            # Truncate if needed\n",
    "            fake_data = fake_data[:, :seq_length]\n",
    "\n",
    "        # Get discriminator's prediction for real and fake data\n",
    "        real_preds = discriminator(inputs)\n",
    "        fake_preds = discriminator(fake_data)\n",
    "\n",
    "        # Define fake labels\n",
    "        fake_labels = torch.zeros(inputs.size(0), 1)\n",
    "\n",
    "        # Compute the loss\n",
    "        real_loss = criterion_disc(real_preds, real_labels)\n",
    "        fake_loss = criterion_disc(fake_preds, fake_labels)\n",
    "\n",
    "        loss = real_loss + fake_loss\n",
    "\n",
    "        # Backpropagate and update discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Collect results for metrics calculation\n",
    "        y_true.extend([1] * real_labels.size(0) + [0] * fake_labels.size(0))\n",
    "        y_pred.extend(torch.cat([real_preds, fake_preds]).round().tolist())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"Discriminator Metrics -> Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# 6. Train the GAN (Adversarial Training)\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "# For BLEU evaluation\n",
    "start_token = train_dataset.word_to_idx[\"<start>\"]\n",
    "id_to_word = train_dataset.idx_to_word\n",
    "\n",
    "def decode_poem(poem_tensor):\n",
    "    return [' '.join([id_to_word[idx.item()] for idx in line if idx.item() != train_dataset.word_to_idx[\"<pad>\"]]) for line in poem_tensor]\n",
    "\n",
    "gan_epochs = 3\n",
    "print(\"Adversarial Training...\")\n",
    "for epoch in range(gan_epochs):\n",
    "    g_loss_epoch = 0\n",
    "    d_loss_epoch = 0\n",
    "\n",
    "    for inputs, _ in tqdm(train_loader):\n",
    "        # Train Discriminator\n",
    "        discriminator.train()\n",
    "\n",
    "        real_labels = torch.ones(inputs.size(0), 1)\n",
    "        fake_data = generator.sample(start_token=start_token, max_length=seq_length, num_lines=inputs.size(0))\n",
    "        fake_data = fake_data.clone().detach()\n",
    "        if fake_data.size(1) < seq_length:\n",
    "            padding_size = (seq_length - fake_data.size(1))\n",
    "            fake_data = torch.nn.functional.pad(fake_data, (0, padding_size), value=train_dataset.word_to_idx[\"<pad>\"])\n",
    "        elif fake_data.size(1) > seq_length:\n",
    "            fake_data = fake_data[:, :seq_length]\n",
    "\n",
    "        real_preds = discriminator(inputs)\n",
    "        fake_preds = discriminator(fake_data)\n",
    "\n",
    "        fake_labels = torch.zeros(inputs.size(0), 1)\n",
    "\n",
    "        real_loss = criterion_disc(real_preds, real_labels)\n",
    "        fake_loss = criterion_disc(fake_preds, fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        d_loss_epoch += d_loss.item()\n",
    "\n",
    "        # Train Generator\n",
    "        generator.train()\n",
    "\n",
    "        fake_data = generator.sample(start_token=start_token, max_length=seq_length, num_lines=inputs.size(0))\n",
    "        \n",
    "        fake_data = fake_data.clone().detach()\n",
    "\n",
    "        if fake_data.size(1) < seq_length:\n",
    "            padding_size = (seq_length - fake_data.size(1))\n",
    "            fake_data = torch.nn.functional.pad(fake_data, (0, padding_size), value=train_dataset.word_to_idx[\"<pad>\"])\n",
    "        elif fake_data.size(1) > seq_length:\n",
    "            fake_data = fake_data[:, :seq_length]\n",
    "\n",
    "        gen_preds = discriminator(fake_data)\n",
    "        g_loss = criterion_disc(gen_preds, real_labels)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_loss_epoch += g_loss.item()\n",
    "\n",
    "    g_losses.append(g_loss_epoch / len(train_loader))\n",
    "    d_losses.append(d_loss_epoch / len(train_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Generator Loss: {g_losses[-1]:.4f}, Discriminator Loss: {d_losses[-1]:.4f}\")\n",
    "\n",
    "\n",
    "# Function to remove unwanted tokens and adjust the length\n",
    "def generate_poem_lines(generator, start_token, train_dataset, num_lines=10, min_words=5, max_words=10):\n",
    "    poem_lines = generator.sample(start_token=start_token, max_length=seq_length, num_lines=num_lines)\n",
    "\n",
    "    poem_words = []\n",
    "    for line in poem_lines:\n",
    "        words = []\n",
    "        used_words = set()  # Track used words in the line to avoid repetition\n",
    "        for idx in line:\n",
    "            word = train_dataset.idx_to_word[idx.item()]  # Convert tensor index to item if needed\n",
    "\n",
    "            # Skip unwanted tokens\n",
    "            if word not in [\"<start>\", \"<unk>\", \"<pad>\"] and word not in used_words:\n",
    "                words.append(word)\n",
    "                used_words.add(word)\n",
    "\n",
    "        # Adjust the line to meet the random word count range\n",
    "        word_count = len(words)\n",
    "        random_word_count = random.randint(min_words, max_words)  # Choose a random word count between 5 and 10\n",
    "\n",
    "        # If fewer than the random word count, extend the line with random words (avoiding repetition)\n",
    "        while len(words) < random_word_count:\n",
    "            additional_word = random.choice([word for word in train_dataset.idx_to_word.values() if word not in used_words and word not in [\"<start>\", \"<unk>\", \"<pad>\"]])\n",
    "            words.append(additional_word)\n",
    "            used_words.add(additional_word)\n",
    "\n",
    "        # Truncate the line if it exceeds the random word count\n",
    "        if len(words) > random_word_count:\n",
    "            words = words[:random_word_count]\n",
    "\n",
    "        poem_words.append(\" \".join(words))\n",
    "\n",
    "    return poem_words\n",
    "\n",
    "# Final BLEU score calculation with smoothing function\n",
    "if len(val_data) > 0:\n",
    "    references = [line.split() for line in val_data[:10]]  # Using first 10 samples as references\n",
    "\n",
    "    # Generate the final poem\n",
    "    final_generated_poem = generate_poem_lines(generator, start_token=train_dataset.word_to_idx[\"<start>\"], train_dataset=train_dataset, num_lines=random.randint(7, 10))\n",
    "    print(f\"Final Generated Poem: {'\\n'.join(final_generated_poem)}\")\n",
    "\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method4  # Choose a smoothing function\n",
    "\n",
    "    for ref, cand in zip(references, final_generated_poem):\n",
    "        try:\n",
    "            score = sentence_bleu([ref], cand.split(), smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU score: {e}\")\n",
    "            bleu_scores.append(0)  # Add a default value on error\n",
    "    \n",
    "    avg_bleu_score = np.mean(bleu_scores)\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Save the Models\n",
    "torch.save(generator.state_dict(), \"generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator.pth\")\n",
    "\n",
    "# 8. Plot Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(gan_epochs), g_losses, label='Generator Loss')\n",
    "plt.plot(range(gan_epochs), d_losses, label='Discriminator Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Generator and Discriminator Losses during Adversarial Training')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
